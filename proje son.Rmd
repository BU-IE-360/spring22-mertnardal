---
title: "360 project"
output: html_document
date: '2022-06-06'
---


```{r class.source = 'fold-hide'}
# code is hidden initially
```
# IE 360 Spring 2022 Project

## Intoduction

The main aim of this project is forecasting hourly solar power KIVANC 2 Solar Power Central from 24 May 2022 to 3 June by developing models and making statistical analysis based on the available data. The data that is used in project consist of hourly temparature, relative humidity, downward shortwave radiation and cloud cover for the 9 different coordinates related to solar plantsâ€™ locations and hourly production of solar power central. The models that are developed for the forecasting uses the available data (at day d data until day d-1 is available and forecast for day d+1 is done based on available data) and ma ke forecasts which consist of 24 predictions (hourly predictions) for the next day. Before model developing process the data is analyzed visually to determine what kind of models can be useful and develop a better understanding on the data.

```{r message=FALSE, class.source='fold-hide'}
require(data.table)
require(lubridate)
require(zoo)
library(forecast)
library(dplyr)
library(tseries)
library(ggplot2)
require(urca)
library(GGally)
data_path="C:/Users/dogad/Desktop/IE360/proje/2022-06-06_production.csv"
production = fread(data_path)
production[,date:=as.Date(date)]

data_path2="C:/Users/dogad/Desktop/IE360/proje/2022-06-06_weather.csv"
l_weather = fread(data_path2)
l_weather[,date:=as.Date(date)]
w_weather=dcast(l_weather,date+hour~variable+lat+lon,value.var="value")
w_weather$month=as.numeric(format(w_weather$date,format="%m"))

data=merge(production,w_weather,by=c("date","hour"))
data$month=as.numeric(format(data$date,format="%m"))

train_start=as.Date('2021-03-31')
test_start= as.Date('2022-03-01')
test_end = as.Date('2022-05-24')
current_date= "2022-06-02"
```

###Hourly Production Plots 

```{r class.source = 'fold-hide',echo=TRUE}
ggplot(data[date>=train_start],aes(x=date,y=production,col=hour))+geom_line()+facet_wrap(~hour)+theme(axis.text.x=element_text(angle=70, hjust=1.4, vjust = 1.4))

```
It is seen that there is no production between 20.00 pm and 6.00 am. There is a decrease in production for every hours in winter which indicates that there is a seasonal behaviour due to changes in the day times at different seasons. 

###Daily Production
```{r class.source = 'fold-hide',echo=TRUE}
daily_series=data[,list(total=mean(production)),list(date)]
ggplot(daily_series,aes(x=date,y=total))+geom_line()
```

Daily production of solar plants are plotted to obtain general information about the data such as the production capacity of the plants and variation. The variation in February 2021-June 2021 is less than  February 2022-June 2022. Moreover, in 2021 the production is less than in 2022 for some months. For example in April 2021 the production is approximately between 5-10 but in April 2022 the values are between 5-15. An explanation for this situation can be the plants may not have used their 100 % capacity. Therefore some of the data can be cut off to obtain a data with more stable variance and a smoother model.

###Autocorrelation of Hourly Production
```{r class.source = 'fold-hide',echo=TRUE}
acf(data$production,lag.max = 60)
```
###Partial Autocorrelation of Hourly Production
```{r class.source = 'fold-hide',echo=TRUE}
pacf(data$production,lag.max = 60)
```
In both plots the lagrest correlation is in lag 1 and lag24. Lag 1 correlation is high in one hour the weather is not seem to change much so the productions are similar. Lag 24 is high because hours indicates Sunâ€™s position (for instance Sunâ€™s light comes with nearly 90 degree between 12.00-14.00 so there is a peak for this hours at every day and no matter what season is Sun does not appears before 6.00 am so the production is 0 for 20.00 - 06.00 no matter what day it is). 

##Approaches

###1)	Linear Regression and Time Series Regressions
Since data has a seasonal behaviour (hourly-monthly-seasonally), using weather data with time-related regressors can be good approach. Models built step by step (by adding regressors) and their performances measured based on adjusted R squared and residual plots. In addition, since the weather data has too many rows, the data is averaged over 9 coordinates and a ggpairs plot is made to analyze regressors better. These models are built to forecast 07.00-19.00 because there is no significant production before 07.00 and after 19.00, the forecasts for these hours are zero. Therefore there is no need to build a model to predict those hours.

```{r class.source = 'fold-hide',echo=TRUE,warning=FALSE}
avgdata=data.table(date=data$date,hour=data$hour,cloud=rowSums(data[,3:11])/9,dswrf=rowSums(data[,12:20])/9,relhumidity=rowSums(data[,21:29])/9,temp=rowSums(data[,30:38])/9,production=data$production,month=as.numeric(format(data$date,format="%m")))
avgweather=data.table(date=w_weather$date,hour=w_weather$hour,cloud=rowSums(data[,3:11])/9,dswrf=rowSums(data[,12:20])/9,relhumidity=rowSums(data[,21:29])/9,temp=rowSums(data[,30:38])/9)
model_avg = lm(production~.-date-hour-month-production,avgdata[hour>=7 & hour <=18])
ggpairs(avgdata)
```
###a) Model with only averaged data

```{r class.source = 'fold-hide',echo=TRUE}
summary(model_avg)
```

####Residual plots of the only averaged data model
```{r class.source = 'fold-hide',echo=TRUE}
checkresiduals(model_avg)
```
Although all of the regressors are significant and are correlated with the production which can be seen in ggpairs plot (correlation matrix), these regressors are not enough to explain the model just by themselves. Residuals are highly correlated due to seasonal behaivour of the data which has already been observed in introduction part. Therefore month and hour information is added to obtain a better model.

###b) Model with averaged data + time related regressors (hour and month) 
```{r class.source = 'fold-hide',echo=TRUE}
model_avg_time = lm(production~.-date-hour-month+as.factor(month)+as.factor(hour)-production,avgdata[hour>=7 & hour <=18])
summary(model_avg_time) #hour and month added
```
####Residual plots of the averaged data+time related regressors (hour and month) model
```{r class.source = 'fold-hide',echo=TRUE}
checkresiduals(model_avg_time)
```
After adding hour and month information adjusted R squared is significantly increased. The autocorrelation between residuals are also decrased significantly but still beyond the boundaries. Adding lagged variables can solve this problem but before that a pacf analysis of the residuals would be helpful to determine which lagged variables should be added because there is a sinusoidal behaviour in the acf plot and correlation is high for all lags. Therefore, to see the pure effect of lagged variables a pacf analysis is made. 

####PACF plot of the residuals
```{r class.source = 'fold-hide',echo=TRUE}
pacf(model_avg_time$residuals) # pacf analysis of residuals
```
The two largest correlation is at lag 1 and lag 12. Since this model is only for 07.00-19.00 the correlation in lag 12 is corresponds to correlation at lag 24 in the original data. The correlation at lag 1 and lag 24 is also observerd before the modelling and the reasons for correlation are explained. After the modelling it is seen that the correlation for these lags still remains which means that the regressors in this model can not fully explain this relationship but adding the lagged variables can solve this. Therefore lag1 and lag24 are added to original data and an autoregressive model is built.

### c) Model with averaged data + time related regressors (hour and month) + lagged variables 
```{r class.source = 'fold-hide',echo=TRUE}
avgdata_lagged <- data.table(avgdata,lag1=c(NA,avgdata$production[1:length(avgdata$production)-1]),lag24=c(rep(NA,24),avgdata$production[1:(length(avgdata$production)-24)]))
#lag1 and lag 24 added

model_avg_time_lag = lm(production~.-date-hour-month+as.factor(month)+as.factor(hour)-production,avgdata_lagged[hour>=7 & hour <=18])
summary(model_avg_time_lag) #lag1 and lag24 added


ForecastReportlm <- function(x_date) {
lm_vec <- rep(NA,12)
  dataf <- avgdata_lagged[date >= train_start & date < x_date & hour >=7 & hour <= 18]
  model1=lm(formula = production~.-date-hour-month+as.factor(month)+as.factor(hour)-production,dataf)
  test_data <- avgdata_lagged[date==as.Date(x_date)+1& hour >=7 & hour <= 18,c("date","hour","cloud",
                        "dswrf","relhumidity","month","temp","lag1","lag24")]
  lm_vec<- predict(model1,data.frame(test_data))
lm_vec <- c(rep(0,7),lm_vec,rep(0,5))
return(array(unlist(lm_vec)))
}
```

#### Residual plots of Model with averaged data + time related regressors (hour and month) + lagged variables
```{r class.source = 'fold-hide',echo=TRUE}
checkresiduals(model_avg_time_lag)
```

Although adjusted R squared increased significantly, has a great value 0.8725 and correlation between residuals decreased there is still significant correlation between residuals. Also, residuals does not seem to have a constant variance (there is a significant increase after 3000). In short, addition of lagged variables leads to better model but does not solve the problems completely. However the fact that using lagged variables improve the model can be an indication of using arima and arima with regressors model would be helpful. The lagged production values does not fix the residuals completely but using them with lagged error variables can lead to a great improvement on the model.

### 2) ARIMA Models
Since regression models didn't give a satisfactory result in terms of residuals, changing to an ARIMA model approach would be a good idea. Thus, first, data must be stationarized. As seen from the above analysis and below graph, daily series is not stationary since it has trend and seasonality. Therefore, it must be detrended and deseasonalized. For this reason, decomposition is done. 
```{r fig.show='hold',echo=FALSE}
traindata = daily_series[date>=train_start &date <= test_start]
tsdata=ts(traindata$total,frequency = 7)
plot(tsdata, main = "Solar Power Production Time Series (Daily)", 
     col="darkgreen", lwd=1,ylab="Amount", xlab="Weeks") 
```
First data is deseasonalized. Resulting plot is shown below:
```{r fig.show='hold',echo=FALSE}
data_dec_multip<-decompose(tsdata,type="multiplicative")
deseasonalized=tsdata/(data_dec_multip$seasonal)
plot(deseasonalized,main="Time Series of deseasonalized Adjusted Production",col="orange") 
```
Likewise, it is detrended. Resulting plot is shown below:
First data is deseasonalized. Resulting plot is shown below:
```{r fig.show='hold',echo=FALSE}
detrend_seasonalized=deseasonalized/(data_dec_multip$trend)
data_random=detrend_seasonalized
ts.plot(data_random, main="Time Series of detrend & deseasonalized Adjusted Production",col="blue")

```

To see if the detrended and deseasonalized data is stationary, KPSS test is performed.
```{r class.source = 'fold-hide',echo=TRUE}
a<- ur.kpss(data_random)
summary(a)

```
Test statistic is way smaller than 10 percent significance level, in other words, test statistic is not significantly large to reject the null hypothesis. Therefore, the null hyphothesis of the series being stationary is not rejected. This data can now be used to fit ARIMA models.
To start the analysis of selecting parameters for ARIMA, ACF an PACF should be plotted.

```{r class.source = 'fold-hide',echo=TRUE}
plot(acf(na.omit(data_random),lag.max=60,plot=FALSE), main = "Autocorrelation of detrend & deseasonalized Adjusted Production",col="black", lwd=1.5, xlab="Lag in Days")
```

```{r class.source = 'fold-hide',echo=TRUE}
plot(pacf(na.omit(data_random),lag.max=60,plot=FALSE), main = "Partial Autocorrelation of detrend & deseasonalized Adjusted Production",col="black", lwd=1.5, xlab="Lag in Days")
```
In this case, ACF and PACF plots don't indicate some certain parameters. Therefore, first, it would be better to use auto.arima to come up with parameters. 
Since it is expected to come up with a prediction for every hours of the day, the best approach is to fit a model to each hour of a day. (i.e. a model for 01:00, a model for 02:00 etc.). Therefore, fitting an auto.arima model for each hour over the time would be a good starting point.

The evaluation of each model is done at the end of the report, using Weighted Mean Percentage Error as the performance measure.

### a) auto.arima model
```{r class.source = 'fold-hide'}
ForecastReportAutoAr <- function(x_date) {

arima_vec <- rep(NA,24)
BIC1<-rep(NA,24)
for (i in 0:23){
  dataf <- data[date >= train_start & date < x_date & hour == i,c("date","production")]
  dataf <- ts(dataf)
  fitted=auto.arima(dataf[,'production'],seasonal = T,trace=F)
  forecasted1=forecast(fitted,h=3)
  arima_vec[i+1]<-forecasted1$mean[3]
}
return(arima_vec)
}
```
Using a loop, an ARIMA model is fit to each hour of a day to forecast the production values for 2 days later. 

### b)auto.arima model with 1 regressor
To see if adding regressors improves the model, we added the cloud cover data of a single coordinate to the auto.arima model and saw that, which was the most significant variable in the regression analysis.

```{r class.source = 'fold-hide'}
ForecastReportArimax <- function(x_date) {
arima_vec_reg <- rep(NA,24)
for(i in 0:23) {
  dataf <- data[date >= train_start & date < x_date & hour == i,c("date","production")]
  dataf <- ts(dataf)
  regressor <- data[date >= train_start & date < x_date & hour == i,c("CLOUD_LOW_LAYER_36.25_33")]
  fitted_reg <- auto.arima(dataf[,'production'],xreg=data.matrix(regressor))
  regforecast <- w_weather[(date==x_date & hour==i) |(date==as.Date(x_date)+1 & hour==i)|date==as.Date(x_date)+2 & hour==i,c("CLOUD_LOW_LAYER_36.25_33")]
  forecasted2=forecast(fitted_reg,xreg=data.matrix(regforecast),h=3)
  arima_vec_reg[i+1] <- forecasted2$mean[3]
}
return(arima_vec_reg)
}

```



### c) auto.arima model with 3 regressors (ARIMAX3)
```{r class.source = 'fold-hide'}
ForecastReportArimax3<- function(x_date) {
arima_vec_reg2 <- rep(NA,24)
for(i in 0:23) {
  dataf <- avgdata[date >= train_start & date < x_date & hour == i,c("date","production")]
  dataf <- ts(dataf)
  regressor <- avgdata[date >= train_start & date < x_date & hour == i,c("cloud","temp","relhumidity")]
  fitted_reg <- auto.arima(dataf[,'production'],xreg=data.matrix(regressor))
  regforecast <- avgweather[(date==x_date & hour==i) |(date==as.Date(x_date)+1 & hour==i |date==as.Date(x_date)+2 & hour==i),c("cloud","temp","relhumidity")]
  forecasted2=forecast(fitted_reg,xreg=data.matrix(regforecast),h=3)
  arima_vec_reg2[i+1] <- forecasted2$mean[3]
}
return(arima_vec_reg2)
}

```
After seeing adding regressors indeed improves the model we added 3 regressors to the model, namely, cloud, temperature and humidity, which were also significant on the linear regression model. Since there are different values for different coordinates, we used the average of these values as regressors. Since this model gave satisfactorily good results, we used this model during the submission period.

### d)ARIMA (2,0,3)
After conducting further analysis, we developed an ARIMA model with parameters, p=2,d=0 and q=3. To fit this model, for each hour of the day, data was needed to be detrended and deseasonalized, thus, data is decomposed for each hour of the day, using a loop. Then, an arima model of ARIMA(2,0,3) is fit to each of the random components. Here, the random component is predicted and the trend and seasonality components which were previously decomposed from the data are added to the forecasted random component to come up with production values.

```{r class.source = 'fold-hide'}
ForecastReportArima <- function(x_date,Order) {
  Forecasted <- c()
  for (i in 7:19){
    df1<-data[date<=x_date]
    #Hour filtering
    df1=df1[df1$hour==i,c("date","production")]
    
    
    
    combined_data_decomp<-decompose(ts(df1$production,freq=7),type="multiplicative")
    
    
    noise<-combined_data_decomp$random
    trend<-combined_data_decomp$trend
    seasonality<-combined_data_decomp$seasonal
    
    nahead=3 # 2 days later prediction
    
    fitted=arima(noise,order=Order)
    
    predicted_noise <- forecast(fitted,h=nahead)$mean[3]
    last_trend_value <-tail(combined_data_decomp$trend[!is.na(combined_data_decomp$trend)],1)
    seasonality<-tail(combined_data_decomp$seasonal[!is.na(combined_data_decomp$seasonal)],1)
    
    ProdForecast<-predicted_noise*last_trend_value*seasonality
    Forecasted<-c(Forecasted,ProdForecast)
    
  }
  return(Forecasted)
}

```

## Results, Conclusions and Future Work
We were asked to evaluate our models based on WMAPE (Weighted Mean Absolute Percentage Error) metric where weights are the mean of the actual data. Our test dates are between March 1 2022 and May 24 2022. 
During the submission period we used ARIMAX3 model which used 3 regressors with auto arima however since the smallest WMAPE value belongs to the last model we developed that is arima with parameters (2,0,3) we choose that as our final model.
To evaluate the models, we also compared the models developed with naive model, which was basically two days lagged data. 
As it is seen from the below table, in general, ARIMA models did slightly better than the linear regression model and the model which gave the lowest WMAPE value is the ARIMA model with parameter set (2,0,3). Therefore, we chose ARIMA(2,0,3) model as our final model.


```{r echo=FALSE, message=FALSE, warning=FALSE, class.source='fold-hide'}
data2<- data[date>=test_start & date <= test_end,c("date","hour","production")]
data2$forecast_arima = rep(0,nrow(data2))
data2$forecast_arimax = rep(0,nrow(data2))
data2$forecast_arimax3 = rep(0,nrow(data2))
data2$forecast_autoar = rep(0,nrow(data2))
data2$forecast_lm = rep(0,nrow(data2))
for(j in (seq.Date(test_start, test_end, by=1))){
  j=as.Date(j)
  data2[date==j & hour >=7 & hour <= 19]$forecast_arima=ForecastReportArima(as.Date(j),c(2,0,3))
  data2[date==j]$forecast_arimax=ForecastReportArimax(as.Date(j))
  data2[date==j]$forecast_arimax3=ForecastReportArimax3(as.Date(j))
  data2[date==j]$forecast_autoar=ForecastReportAutoAr(as.Date(j))
  data2[date==j]$forecast_lm=ForecastReportlm(as.Date(j))
  
}
naive = data[,c("date","hour","production")] 
naive$naive=lag(naive$production,48)
naive<- naive[date>=test_start & date <= test_end]
data2$naive=naive$naive
accu=function(actual,forecast){
  n=length(actual)
  error=actual-forecast
  mean=mean(actual)
  sd=sd(actual)
  CV=sd/mean
  FBias=sum(error)/sum(actual)
  MAPE=sum(abs(error/actual))/n
  RMSE=sqrt(sum(error^2)/n)
  MAD=sum(abs(error))/n
  MADP=sum(abs(error))/sum(abs(actual))
  WMAPE=MAD/mean
  l=data.frame(n,mean,sd,CV,FBias,MAPE,RMSE,MAD,MADP,WMAPE)
  return(l)
}

melted_result=melt(data2,c('date','hour','production'),c('forecast_arima','forecast_arimax','forecast_arimax3',
                                                         'forecast_autoar','forecast_lm','naive'))

a<-melted_result[,accu(production,value),by=list(variable)]
a

```

Overall, very limited data was available for this analysis, for example, since the plant hadn't reached its full capacity in March 2021, the forecasts for March 2022 were worse than those of May 2022 since in May 2021, plant was at its capacity. Also, further analysis could be conducted on each model for the hours and check which models are contributing to the WMAPE the most. 
